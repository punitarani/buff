{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Citations Analysis using Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f74fdb879a8f4921"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize ChromaDB"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f294e99b2cafbc84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "from config import DATA_DIR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:09:53.761193100Z",
     "start_time": "2024-03-07T03:09:51.458124300Z"
    }
   },
   "id": "58e19cf1a40c44b8",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chroma_db_path = DATA_DIR.joinpath(\"chroma\").resolve()\n",
    "\n",
    "chroma = chromadb.PersistentClient(path=str(chroma_db_path))\n",
    "embeddings = chroma.get_or_create_collection(\"analysis\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:09:54.384517400Z",
     "start_time": "2024-03-07T03:09:53.767195800Z"
    }
   },
   "id": "17a12d1790ba13d3",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38144e01ecb06796"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:09:58.599312900Z",
     "start_time": "2024-03-07T03:09:54.372518800Z"
    }
   },
   "outputs": [],
   "source": [
    "from buff.openalex import Work\n",
    "from buff.openalex.download import get_paper_text\n",
    "from buff.llm.utils import get_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Papers\n",
    "\n",
    "**Paper 2 - Cites - Paper 1**\n",
    "$\\text{Paper 2} \\rightarrow \\text{Paper 1}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6840d384cabf3da7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "paper_1 = \"W2994792393\"\n",
    "paper_2 = \"W3190631166\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:09:58.623682200Z",
     "start_time": "2024-03-07T03:09:58.601110400Z"
    }
   },
   "id": "24a1b17b999d08ef",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "work_1 = await Work(paper_1).data\n",
    "work_2 = await Work(paper_2).data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:09:59.183609600Z",
     "start_time": "2024-03-07T03:09:58.622681300Z"
    }
   },
   "id": "1935aa4cf77feb8d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1:  The autophagy receptor p62/SQST-1 promotes proteostasis and longevity in C. elegans by inducing autophagy\n",
      "Paper 2:  Autophagy in healthy aging and disease\n"
     ]
    }
   ],
   "source": [
    "print(\"Paper 1: \", work_1.title)\n",
    "print(\"Paper 2: \", work_2.title)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:09:59.253518Z",
     "start_time": "2024-03-07T03:09:59.177820500Z"
    }
   },
   "id": "4f91d6d1b4158f52",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1 loaded: 22380 tokens\n",
      "Paper 2 loaded: 32836 tokens\n"
     ]
    }
   ],
   "source": [
    "text_1 = await get_paper_text(work=work_1)\n",
    "text_2 = await get_paper_text(work=work_2)\n",
    "\n",
    "print(f\"Paper 1 loaded: {get_token_count(text_1)} tokens\")\n",
    "print(f\"Paper 2 loaded: {get_token_count(text_2)} tokens\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:10:00.198791600Z",
     "start_time": "2024-03-07T03:09:59.200960Z"
    }
   },
   "id": "a35bfa457a551e1c",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7765166fc0b4ccd9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from buff.llm.split import split_text\n",
    "# \n",
    "# text_1_chunks = split_text(text_1)\n",
    "# text_2_chunks = split_text(text_2)\n",
    "# \n",
    "# # Save the chunks\n",
    "# with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(json.dumps({\n",
    "#         \"paper_1\": text_1_chunks,\n",
    "#         \"paper_2\": text_2_chunks\n",
    "#     }))\n",
    "#     print(\"Chunks saved\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2ef8324d1846087"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks saved\n",
      "Paper 1 split into 42 chunks\n",
      "Paper 2 split into 66 chunks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the chunks\n",
    "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_chunks = json.load(f)\n",
    "    text_1_chunks = text_chunks[\"paper_1\"]\n",
    "    text_2_chunks = text_chunks[\"paper_2\"]\n",
    "    print(\"Chunks loaded\")\n",
    "\n",
    "print(f\"Paper 1 split into {len(text_1_chunks)} chunks\")\n",
    "print(f\"Paper 2 split into {len(text_2_chunks)} chunks\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:10:16.039402900Z",
     "start_time": "2024-03-07T03:10:10.484102700Z"
    }
   },
   "id": "8df92a5693aff886",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print min-mean-max token count for each chunk"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b07866665594cea"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1 chunk stats: 267 - 532 - 815\n",
      "Paper 2 chunk stats: 81 - 497 - 684\n"
     ]
    }
   ],
   "source": [
    "text_1_chunks_tokens = [get_token_count(chunk) for chunk in text_1_chunks]\n",
    "text_2_chunks_tokens = [get_token_count(chunk) for chunk in text_2_chunks]\n",
    "\n",
    "print(\n",
    "    f\"Paper 1 chunk stats: {min(text_1_chunks_tokens)} - {sum(text_1_chunks_tokens) // len(text_1_chunks_tokens)} - {max(text_1_chunks_tokens)}\")\n",
    "print(\n",
    "    f\"Paper 2 chunk stats: {min(text_2_chunks_tokens)} - {sum(text_2_chunks_tokens) // len(text_2_chunks_tokens)} - {max(text_2_chunks_tokens)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:10:16.224031100Z",
     "start_time": "2024-03-07T03:10:16.042397100Z"
    }
   },
   "id": "82b35dc48f356245",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26196d1b185416a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved\n",
      "Paper 1 embedded into 42 chunks\n",
      "Paper 2 embedded into 66 chunks\n"
     ]
    }
   ],
   "source": [
    "# from buff.llm.embed import embed_texts\n",
    "# \n",
    "# text_1_embeddings = await embed_texts(text_1_chunks)\n",
    "# text_2_embeddings = await embed_texts(text_2_chunks)\n",
    "# \n",
    "# # Save the embeddings\n",
    "# with open(\"embeddings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(json.dumps({\n",
    "#         \"paper_1\": text_1_embeddings,\n",
    "#         \"paper_2\": text_2_embeddings\n",
    "#     }))\n",
    "#     print(\"Embeddings saved\")\n",
    "\n",
    "# Load the embeddings\n",
    "with open(\"embeddings.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_embeddings = json.load(f)\n",
    "    text_1_embeddings = text_embeddings[\"paper_1\"]\n",
    "    text_2_embeddings = text_embeddings[\"paper_2\"]\n",
    "    print(\"Embeddings loaded\")\n",
    "\n",
    "print(f\"Paper 1 embedded into {len(text_1_embeddings)} chunks\")\n",
    "print(f\"Paper 2 embedded into {len(text_2_embeddings)} chunks\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:10:18.234424400Z",
     "start_time": "2024-03-07T03:10:16.233029900Z"
    }
   },
   "id": "cc89915b5b9871e1",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Store the documents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad0eeec6162a4e43"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1: 42 documents\n",
      "Paper 2: 66 documents\n",
      "Total: 108 documents.\n",
      "Embeddings stored\n"
     ]
    }
   ],
   "source": [
    "# from buff.llm.models import Document, DocumentMetadata\n",
    "# \n",
    "# # Build the documents\n",
    "# text_1_documents = [\n",
    "#     Document(\n",
    "#         metadata=DocumentMetadata(\n",
    "#             index=i,\n",
    "#             title=work_1.title,\n",
    "#             work_id=str(work_1.id),\n",
    "#             doi=str(work_1.doi),\n",
    "#             text=text_1_chunks[i]\n",
    "#         ),\n",
    "#         id=f\"{work_1.id}-{i}\",\n",
    "#         values=text_1_embeddings[i]\n",
    "#     )\n",
    "#     for i in range(len(text_1_chunks))\n",
    "# ]\n",
    "# \n",
    "# text_2_documents = [\n",
    "#     Document(\n",
    "#         metadata=DocumentMetadata(\n",
    "#             index=i,\n",
    "#             title=work_2.title,\n",
    "#             work_id=str(work_2.id),\n",
    "#             doi=str(work_2.doi),\n",
    "#             text=text_2_chunks[i]\n",
    "#         ),\n",
    "#         id=f\"{work_2.id}-{i}\",\n",
    "#         values=text_2_embeddings[i]\n",
    "#     )\n",
    "#     for i in range(len(text_2_chunks))\n",
    "# ]\n",
    "# print(f\"Paper 1: {len(text_1_documents)} documents\")\n",
    "# print(f\"Paper 2: {len(text_2_documents)} documents\")\n",
    "# \n",
    "# \n",
    "# # Create the following lists of data from the 2 documents arrays\n",
    "# text_chunk_ids = [doc.id for doc in text_1_documents + text_2_documents]\n",
    "# text_chunk_embeddings = [doc.values for doc in text_1_documents + text_2_documents]\n",
    "# text_chunk_metadata = [doc.metadata.model_dump(mode=\"json\") for doc in text_1_documents + text_2_documents]\n",
    "# print(f\"Total: {len(text_chunk_ids)} documents.\")\n",
    "# if not len(text_chunk_embeddings) == len(text_chunk_metadata) == len(text_chunk_ids):\n",
    "#     raise ValueError(\"Invalid data\")\n",
    "# \n",
    "# # Store the embeddings\n",
    "# embeddings.add(\n",
    "#     ids=text_chunk_ids,\n",
    "#     embeddings=text_chunk_embeddings,\n",
    "#     metadatas=text_chunk_metadata\n",
    "# )\n",
    "# print(\"Embeddings stored\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T03:12:20.179948400Z",
     "start_time": "2024-03-07T03:12:19.414582500Z"
    }
   },
   "id": "c371b3468384df2",
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
